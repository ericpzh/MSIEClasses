{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "60000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nnet = Net()\\n\\nnumparams = 0\\nfor f in net.parameters():\\n    print(f.size())\\n    numparams += f.numel()\\nprint(\"NO.\")\\nprint(numparams)\\n\\noptimizer = optim.SGD(net.parameters(), lr=0.1, weight_decay=0)\\noptimizer.zero_grad()\\n\\ncriterion = nn.CrossEntropyLoss()\\n\\ndef test(net, testLoader):\\n    net.eval()\\n    correct = 0\\n    with torch.no_grad():\\n        for (data,target) in testLoader:\\n            output = net(data)\\n            pred = output.max(1, keepdim=True)[1]\\n            correct += pred.eq(target.view_as(pred)).sum().item()\\n            \\n        print(\"Test Accuracy: %f\" % (100.*correct/len(testLoader.dataset)))\\n\\ntest(net, testLoader)\\n\\nfor epoch in range(10):\\n    net.train()\\n    for batch_idx, (data, target) in enumerate(trainLoader):\\n        pred = net(data)\\n        loss = criterion(pred, target)\\n        loss.backward()\\n        gn = 0\\n        for f in net.parameters():\\n            gn = gn + torch.norm(f.grad)\\n        #print(\"E: %d; B: %d; Loss: %f; ||g||: %f\" % (epoch, batch_idx, loss, gn))\\n        optimizer.step()\\n        optimizer.zero_grad()\\n    \\n    test(net, testLoader)\\n\\nprint(numparams)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import struct\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "class OurDataset(Dataset):\n",
    "    def __init__(self, fnData, fnLabels, transform=None):\n",
    "        self.transform = transform\n",
    "        self.LoadData(fnData)\n",
    "        self.LoadLabels(fnLabels)\n",
    "        assert self.l.size()[0]==self.d.size()[0]\n",
    "    \n",
    "    def LoadLabels(self, fnLabels):\n",
    "        fid = open(fnLabels,'rb')\n",
    "        head = fid.read(8)\n",
    "        data = fid.read()\n",
    "        fid.close()\n",
    "\n",
    "        res = struct.unpack('>ii',head)\n",
    "        data1 = struct.unpack(\">\"+\"B\"*res[1],data)\n",
    "        self.l = torch.LongTensor(data1)\n",
    "\n",
    "    def LoadData(self, fnData):\n",
    "        fid = open(fnData,'rb')\n",
    "        head = fid.read(16)\n",
    "        data = fid.read()\n",
    "        fid.close()\n",
    "\n",
    "        res = struct.unpack(\">iiii\", head)\n",
    "        data1 = struct.iter_unpack(\">\"+\"B\"*784,data)\n",
    "\n",
    "        self.d = torch.zeros(res[1],1,res[2],res[3])\n",
    "        for idx,k in enumerate(data1):\n",
    "            tmp = torch.Tensor(k)\n",
    "            tmp = tmp.view(1,res[2],res[3])\n",
    "            if self.transform:\n",
    "                tmp = self.transform(tmp)\n",
    "            self.d[idx,:,:,:] = tmp\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.d.size()[0]\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.d[idx,:,:], self.l[idx])\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        ##############################\n",
    "        ## declare the layers of the network which have parameters\n",
    "        ##############################\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)#in_chan, out_chan, filter_size, stride\n",
    "        self.fc1 = nn.Linear(50*4*4, 50, bias=True)#50*4*4 -> 50\n",
    "        self.fc2 = nn.Linear(50, 10, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ##############################\n",
    "        ## combine the layers; don't forget the relu and pooling operations\n",
    "        ##############################\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 50*4*4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "testData = OurDataset('MNIST/t10k-images-idx3-ubyte','MNIST/t10k-labels-idx1-ubyte',transform=transforms.Compose([\n",
    "                           transforms.Normalize((255*0.1307,), (255*0.3081,))\n",
    "                       ]))\n",
    "\n",
    "\n",
    "trainData = OurDataset('MNIST/train-images-idx3-ubyte','MNIST/train-labels-idx1-ubyte',transform=transforms.Compose([\n",
    "                           transforms.Normalize((255*0.1307,), (255*0.3081,))\n",
    "                       ]))\n",
    "print(testData.__len__())\n",
    "print(trainData.__len__())\n",
    "\n",
    "trainLoader = DataLoader(trainData, batch_size=128, shuffle=True, num_workers=0)\n",
    "testLoader = DataLoader(testData, batch_size=128, shuffle=False, num_workers=0)\n",
    "'''\n",
    "net = Net()\n",
    "\n",
    "numparams = 0\n",
    "for f in net.parameters():\n",
    "    print(f.size())\n",
    "    numparams += f.numel()\n",
    "print(\"NO.\")\n",
    "print(numparams)\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1, weight_decay=0)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def test(net, testLoader):\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for (data,target) in testLoader:\n",
    "            output = net(data)\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            \n",
    "        print(\"Test Accuracy: %f\" % (100.*correct/len(testLoader.dataset)))\n",
    "\n",
    "test(net, testLoader)\n",
    "\n",
    "for epoch in range(10):\n",
    "    net.train()\n",
    "    for batch_idx, (data, target) in enumerate(trainLoader):\n",
    "        pred = net(data)\n",
    "        loss = criterion(pred, target)\n",
    "        loss.backward()\n",
    "        gn = 0\n",
    "        for f in net.parameters():\n",
    "            gn = gn + torch.norm(f.grad)\n",
    "        #print(\"E: %d; B: %d; Loss: %f; ||g||: %f\" % (epoch, batch_idx, loss, gn))\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    test(net, testLoader)\n",
    "\n",
    "print(numparams)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.6450,\n",
      "           1.9305,  1.5996,  1.4978,  0.3395,  0.0340, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  2.4015,\n",
      "           2.8088,  2.8088,  2.8088,  2.8088,  2.6433,  2.0960,  2.0960,\n",
      "           2.0960,  2.0960,  2.0960,  2.0960,  2.0960,  2.0960,  1.7396,\n",
      "           0.2377, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.4286,\n",
      "           1.0268,  0.4922,  1.0268,  1.6505,  2.4651,  2.8088,  2.4396,\n",
      "           2.8088,  2.8088,  2.8088,  2.7578,  2.4906,  2.8088,  2.8088,\n",
      "           1.3577, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.2078,  0.4159, -0.2460,\n",
      "           0.4286,  0.4286,  0.4286,  0.3268, -0.1569,  2.5797,  2.8088,\n",
      "           0.9250, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242,  0.6322,  2.7960,  2.2360,\n",
      "          -0.1951, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.1442,  2.5415,  2.8215,  0.6322,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242,  1.2177,  2.8088,  2.6051,  0.1358,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242,  0.3268,  2.7451,  2.8088,  0.3649, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242,  1.2686,  2.8088,  1.9560, -0.3606, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.3097,  2.1851,  2.7324,  0.3140, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242,  1.1795,  2.8088,  1.8923, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "           0.5304,  2.7706,  2.6306,  0.3013, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.1824,\n",
      "           2.3887,  2.8088,  1.6887, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.3860,  2.1596,\n",
      "           2.8088,  2.3633,  0.0213, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.0595,  2.8088,\n",
      "           2.8088,  0.5559, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.0296,  2.4269,  2.8088,\n",
      "           1.0395, -0.4115, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242,  1.2686,  2.8088,  2.8088,\n",
      "           0.2377, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242,  0.3522,  2.6560,  2.8088,  2.8088,\n",
      "           0.2377, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242,  1.1159,  2.8088,  2.8088,  2.3633,\n",
      "           0.0849, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242,  1.1159,  2.8088,  2.2105, -0.1951,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
      "         [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "          -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242]]]), tensor(7))\n"
     ]
    }
   ],
   "source": [
    "print(testData.__getitem__(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
