{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import struct\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "class OurDataset(Dataset):\n",
    "    def __init__(self, fnData, fnLabels, transform=None):\n",
    "        self.transform = transform\n",
    "        self.LoadData(fnData)\n",
    "        self.LoadLabels(fnLabels)\n",
    "        assert self.l.size()[0]==self.d.size()[0]\n",
    "    \n",
    "    def LoadLabels(self, fnLabels):\n",
    "        fid = open(fnLabels,'rb')\n",
    "        head = fid.read(8)\n",
    "        data = fid.read()\n",
    "        fid.close()\n",
    "\n",
    "        res = struct.unpack('>ii',head)\n",
    "        data1 = struct.unpack(\">\"+\"B\"*res[1],data)\n",
    "        self.l = torch.LongTensor(data1)\n",
    "\n",
    "    def LoadData(self, fnData):\n",
    "        fid = open(fnData,'rb')\n",
    "        head = fid.read(16)\n",
    "        data = fid.read()\n",
    "        fid.close()\n",
    "\n",
    "        res = struct.unpack(\">iiii\", head)\n",
    "        data1 = struct.iter_unpack(\">\"+\"B\"*784,data)\n",
    "\n",
    "        self.d = torch.zeros(res[1],1,res[2],res[3])\n",
    "        for idx,k in enumerate(data1):\n",
    "            tmp = torch.Tensor(k)\n",
    "            tmp = tmp.view(1,res[2],res[3])\n",
    "            if self.transform:\n",
    "                tmp = self.transform(tmp)\n",
    "            self.d[idx,:,:,:] = tmp\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.d.size()[0]\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.d[idx,:,:], self.l[idx])\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        ##############################\n",
    "        ## declare the layers of the network which have parameters\n",
    "        ##############################\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)#in_chan, out_chan, filter_size, stride\n",
    "        self.fc1 = nn.Linear(50*4*4, 50, bias=True)#50*4*4 -> 50\n",
    "        self.fc2 = nn.Linear(50, 10, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ##############################\n",
    "        ## combine the layers; don't forget the relu and pooling operations\n",
    "        ##############################\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 50*4*4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "testData = OurDataset('MNIST/t10k-images-idx3-ubyte','MNIST/t10k-labels-idx1-ubyte',transform=transforms.Compose([\n",
    "                           transforms.Normalize((255*0.1307,), (255*0.3081,))\n",
    "                       ]))\n",
    "trainData = OurDataset('MNIST/train-images-idx3-ubyte','MNIST/train-labels-idx1-ubyte',transform=transforms.Compose([\n",
    "                           transforms.Normalize((255*0.1307,), (255*0.3081,))\n",
    "                       ]))\n",
    "print(testData.__len__())\n",
    "print(trainData.__len__())\n",
    "\n",
    "trainLoader = DataLoader(trainData, batch_size=128, shuffle=True, num_workers=0)\n",
    "testLoader = DataLoader(testData, batch_size=128, shuffle=False, num_workers=0)\n",
    "\n",
    "net = Net()\n",
    "\n",
    "numparams = 0\n",
    "for f in net.parameters():\n",
    "    print(f.size())\n",
    "    numparams += f.numel()\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1, weight_decay=0)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def test(net, testLoader):\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for (data,target) in testLoader:\n",
    "            output = net(data)\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            \n",
    "        print(\"Test Accuracy: %f\" % (100.*correct/len(testLoader.dataset)))\n",
    "\n",
    "test(net, testLoader)\n",
    "\n",
    "for epoch in range(10):\n",
    "    net.train()\n",
    "    for batch_idx, (data, target) in enumerate(trainLoader):\n",
    "        pred = net(data)\n",
    "        loss = criterion(pred, target)\n",
    "        loss.backward()\n",
    "        gn = 0\n",
    "        for f in net.parameters():\n",
    "            gn = gn + torch.norm(f.grad)\n",
    "        #print(\"E: %d; B: %d; Loss: %f; ||g||: %f\" % (epoch, batch_idx, loss, gn))\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    test(net, testLoader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
