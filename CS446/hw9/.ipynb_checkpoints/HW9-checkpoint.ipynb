{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "E: 0; B: 0; DLoss: 0.686484\n",
      "tensor([[-0.0031],\n",
      "        [-0.0032],\n",
      "        [-0.0029],\n",
      "        [-0.0027],\n",
      "        [-0.0035],\n",
      "        [-0.0035],\n",
      "        [-0.0040],\n",
      "        [-0.0033],\n",
      "        [-0.0030],\n",
      "        [-0.0023],\n",
      "        [-0.0031],\n",
      "        [-0.0034],\n",
      "        [-0.0031],\n",
      "        [-0.0037],\n",
      "        [-0.0028],\n",
      "        [-0.0038],\n",
      "        [-0.0034],\n",
      "        [-0.0040],\n",
      "        [-0.0029],\n",
      "        [-0.0027],\n",
      "        [-0.0028],\n",
      "        [-0.0037],\n",
      "        [-0.0034],\n",
      "        [-0.0029],\n",
      "        [-0.0033],\n",
      "        [-0.0031],\n",
      "        [-0.0034],\n",
      "        [-0.0033],\n",
      "        [-0.0032],\n",
      "        [-0.0035],\n",
      "        [-0.0038],\n",
      "        [-0.0036],\n",
      "        [-0.0036],\n",
      "        [-0.0038],\n",
      "        [-0.0032],\n",
      "        [-0.0019],\n",
      "        [-0.0035],\n",
      "        [-0.0033],\n",
      "        [-0.0032],\n",
      "        [-0.0037],\n",
      "        [-0.0035],\n",
      "        [-0.0037],\n",
      "        [-0.0030],\n",
      "        [-0.0029],\n",
      "        [-0.0037],\n",
      "        [-0.0031],\n",
      "        [-0.0033],\n",
      "        [-0.0033],\n",
      "        [-0.0039],\n",
      "        [-0.0034],\n",
      "        [-0.0038],\n",
      "        [-0.0027],\n",
      "        [-0.0037],\n",
      "        [-0.0029],\n",
      "        [-0.0033],\n",
      "        [-0.0037],\n",
      "        [-0.0033],\n",
      "        [-0.0037],\n",
      "        [-0.0031],\n",
      "        [-0.0020],\n",
      "        [-0.0037],\n",
      "        [-0.0038],\n",
      "        [-0.0036],\n",
      "        [-0.0037],\n",
      "        [-0.0030],\n",
      "        [-0.0025],\n",
      "        [-0.0029],\n",
      "        [-0.0034],\n",
      "        [-0.0028],\n",
      "        [-0.0034],\n",
      "        [-0.0036],\n",
      "        [-0.0028],\n",
      "        [-0.0031],\n",
      "        [-0.0038],\n",
      "        [-0.0030],\n",
      "        [-0.0040],\n",
      "        [-0.0030],\n",
      "        [-0.0032],\n",
      "        [-0.0026],\n",
      "        [-0.0033],\n",
      "        [-0.0028],\n",
      "        [-0.0032],\n",
      "        [-0.0032],\n",
      "        [-0.0034],\n",
      "        [-0.0029],\n",
      "        [-0.0044],\n",
      "        [-0.0031],\n",
      "        [-0.0032],\n",
      "        [-0.0035],\n",
      "        [-0.0035],\n",
      "        [-0.0031],\n",
      "        [-0.0030],\n",
      "        [-0.0044],\n",
      "        [-0.0034],\n",
      "        [-0.0038],\n",
      "        [-0.0040],\n",
      "        [-0.0032],\n",
      "        [-0.0041],\n",
      "        [-0.0032],\n",
      "        [-0.0035],\n",
      "        [-0.0031],\n",
      "        [-0.0036],\n",
      "        [-0.0035],\n",
      "        [-0.0031],\n",
      "        [-0.0032],\n",
      "        [-0.0040],\n",
      "        [-0.0036],\n",
      "        [-0.0031],\n",
      "        [-0.0034],\n",
      "        [-0.0036],\n",
      "        [-0.0034],\n",
      "        [-0.0040],\n",
      "        [-0.0034],\n",
      "        [-0.0039],\n",
      "        [-0.0026],\n",
      "        [-0.0027],\n",
      "        [-0.0039],\n",
      "        [-0.0039],\n",
      "        [-0.0035],\n",
      "        [-0.0037],\n",
      "        [-0.0040],\n",
      "        [-0.0037],\n",
      "        [-0.0033],\n",
      "        [-0.0035],\n",
      "        [-0.0031],\n",
      "        [-0.0033],\n",
      "        [-0.0035],\n",
      "        [-0.0031],\n",
      "        [-0.0036],\n",
      "        [-0.0030],\n",
      "        [-0.0036],\n",
      "        [-0.0033],\n",
      "        [-0.0035],\n",
      "        [-0.0031],\n",
      "        [-0.0030],\n",
      "        [-0.0026],\n",
      "        [-0.0034],\n",
      "        [-0.0027],\n",
      "        [-0.0037],\n",
      "        [-0.0041],\n",
      "        [-0.0037],\n",
      "        [-0.0038],\n",
      "        [-0.0030],\n",
      "        [-0.0032],\n",
      "        [-0.0034],\n",
      "        [-0.0034],\n",
      "        [-0.0032],\n",
      "        [-0.0037],\n",
      "        [-0.0024],\n",
      "        [-0.0037],\n",
      "        [-0.0028],\n",
      "        [-0.0027],\n",
      "        [-0.0035],\n",
      "        [-0.0030],\n",
      "        [-0.0031],\n",
      "        [-0.0029],\n",
      "        [-0.0033],\n",
      "        [-0.0031],\n",
      "        [-0.0041],\n",
      "        [-0.0032],\n",
      "        [-0.0037],\n",
      "        [-0.0034],\n",
      "        [-0.0038],\n",
      "        [-0.0028],\n",
      "        [-0.0030],\n",
      "        [-0.0035],\n",
      "        [-0.0031],\n",
      "        [-0.0033],\n",
      "        [-0.0034],\n",
      "        [-0.0030],\n",
      "        [-0.0030],\n",
      "        [-0.0040],\n",
      "        [-0.0036],\n",
      "        [-0.0034],\n",
      "        [-0.0036],\n",
      "        [-0.0025],\n",
      "        [-0.0035],\n",
      "        [-0.0036],\n",
      "        [-0.0032],\n",
      "        [-0.0035],\n",
      "        [-0.0035],\n",
      "        [-0.0030],\n",
      "        [-0.0032],\n",
      "        [-0.0035],\n",
      "        [-0.0034],\n",
      "        [-0.0030],\n",
      "        [-0.0039],\n",
      "        [-0.0039],\n",
      "        [-0.0038],\n",
      "        [-0.0030],\n",
      "        [-0.0033],\n",
      "        [-0.0034],\n",
      "        [-0.0036],\n",
      "        [-0.0038],\n",
      "        [-0.0032],\n",
      "        [-0.0022],\n",
      "        [-0.0041],\n",
      "        [-0.0029],\n",
      "        [-0.0033],\n",
      "        [-0.0039],\n",
      "        [-0.0029],\n",
      "        [-0.0037],\n",
      "        [-0.0036],\n",
      "        [-0.0040],\n",
      "        [-0.0031],\n",
      "        [-0.0032],\n",
      "        [-0.0031],\n",
      "        [-0.0030],\n",
      "        [-0.0034],\n",
      "        [-0.0029],\n",
      "        [-0.0036],\n",
      "        [-0.0028],\n",
      "        [-0.0030],\n",
      "        [-0.0035],\n",
      "        [-0.0032],\n",
      "        [-0.0042],\n",
      "        [-0.0036],\n",
      "        [-0.0030],\n",
      "        [-0.0040],\n",
      "        [-0.0035],\n",
      "        [-0.0029],\n",
      "        [-0.0035],\n",
      "        [-0.0034],\n",
      "        [-0.0026],\n",
      "        [-0.0034],\n",
      "        [-0.0036],\n",
      "        [-0.0036],\n",
      "        [-0.0031],\n",
      "        [-0.0030],\n",
      "        [-0.0031],\n",
      "        [-0.0032],\n",
      "        [-0.0034],\n",
      "        [-0.0027],\n",
      "        [-0.0031],\n",
      "        [-0.0033],\n",
      "        [-0.0037],\n",
      "        [-0.0033],\n",
      "        [-0.0034],\n",
      "        [-0.0033],\n",
      "        [-0.0032],\n",
      "        [-0.0028],\n",
      "        [-0.0036],\n",
      "        [-0.0036],\n",
      "        [-0.0031],\n",
      "        [-0.0035],\n",
      "        [-0.0031],\n",
      "        [-0.0030],\n",
      "        [-0.0035],\n",
      "        [-0.0031],\n",
      "        [-0.0034],\n",
      "        [-0.0041],\n",
      "        [-0.0032],\n",
      "        [-0.0024],\n",
      "        [-0.0026],\n",
      "        [-0.0030],\n",
      "        [-0.0032]], grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ericp\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-ed1211a40ed8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    174\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m             \u001b[1;31m#loss = criterion(logit, target2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"E: %d; B: %d; GLoss: %f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m             \u001b[0mgopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \"\"\"\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %load A9_GAN.py\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "import struct\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "#matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.manual_seed(2)\n",
    "\n",
    "class OurDataset(Dataset):\n",
    "    def __init__(self, fnData, dev, transform=None):\n",
    "        self.transform = transform\n",
    "        self.LoadData(fnData, dev)\n",
    "\n",
    "    def LoadData(self, fnData, dev):\n",
    "        fid = open(fnData,'rb')\n",
    "        head = fid.read(16)\n",
    "        data = fid.read()\n",
    "        fid.close()\n",
    "\n",
    "        res = struct.unpack(\">iiii\", head)\n",
    "        data1 = struct.iter_unpack(\">\"+\"B\"*784,data)\n",
    "\n",
    "        self.d = torch.zeros(res[1],1,res[2],res[3])\n",
    "        for idx,k in enumerate(data1):\n",
    "            tmp = torch.Tensor(k)\n",
    "            tmp = tmp.view(1,res[2],res[3])\n",
    "            if self.transform:\n",
    "                tmp = self.transform(tmp)\n",
    "            self.d[idx,:,:,:] = tmp\n",
    "\n",
    "        self.d = self.d.to(dev)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.d.size()[0]\n",
    "    def __getitem__(self, idx):\n",
    "        return self.d[idx,:,:]\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 2, kernel_size=3, stride=1, padding=1) #out: 28 -> 14\n",
    "        self.conv2 = nn.Conv2d(2, 4, kernel_size=3, stride=1, padding=1) #out: 14 -> 7\n",
    "        self.conv3 = nn.Conv2d(4, 8, kernel_size=3, stride=1, padding=0) #out: 5 -> 5\n",
    "        self.fc1 = nn.Linear(5*5*8, 16)\n",
    "        self.fc2 = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(-1, 5*5*8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "class Gen(nn.Module):\n",
    "    def __init__(self, zdim):\n",
    "        super(Gen, self).__init__()\n",
    "        self.firstDim = 16\n",
    "        self.fc1 = nn.Linear(zdim, 4*4*self.firstDim)\n",
    "        self.conv1 = nn.ConvTranspose2d( self.firstDim,  8, kernel_size=4, stride=2, padding=2, bias=False) #out: 6\n",
    "        self.conv2 = nn.ConvTranspose2d( 8,  4, kernel_size=4, stride=2, padding=0, bias=False) #out: 14\n",
    "        self.conv3 = nn.ConvTranspose2d( 4,  2, kernel_size=4, stride=2, padding=1, bias=False) #out: 28\n",
    "        self.conv4 = nn.Conv2d(2, 1, kernel_size=3, padding=1, bias=False)\n",
    "        self.fc2 = nn.Linear(1, 784, bias=False)\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = torch.nn.LeakyReLU(0.2)(self.fc1(z))\n",
    "        z = z.view(z.size()[0],self.firstDim,4,4)\n",
    "        z = torch.nn.LeakyReLU(0.2)(self.conv1(z))\n",
    "        z = torch.nn.LeakyReLU(0.2)(self.conv2(z))\n",
    "        z = torch.nn.LeakyReLU(0.2)(self.conv3(z))\n",
    "        z = F.sigmoid(self.conv4(z))\n",
    "        return z\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, torch.nn.Conv2d):\n",
    "        #print(torch.typename(m))\n",
    "        torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "        #m.weight.data.fill_(0.01)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "        #print(torch.typename(m))\n",
    "        torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "        #m.weight.data.fill_(0.01)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "    if isinstance(m, torch.nn.ConvTranspose2d):\n",
    "        #print(torch.typename(m))\n",
    "        torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "        #m.weight.data.fill_(0.01)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "\n",
    "trainData = OurDataset('./data/train-images-idx3-ubyte', dev, transform=transforms.Compose([\n",
    "                           transforms.Normalize((255*0.,), (255.*1.0,))\n",
    "                       ]))\n",
    "print(trainData.__len__())\n",
    "\n",
    "discIter = 1\n",
    "genIter = 1\n",
    "numEpoch = 250\n",
    "batchSize = 256\n",
    "zdim = 64\n",
    "\n",
    "trainLoader = DataLoader(trainData, batch_size=batchSize, shuffle=True, num_workers=0, drop_last=True)\n",
    "\n",
    "disc = Net().to(dev)\n",
    "gen = Gen(zdim).to(dev)\n",
    "\n",
    "disc.apply(weights_init)\n",
    "gen.apply(weights_init)\n",
    "\n",
    "dopt = optim.Adam(disc.parameters(), lr=0.0002, weight_decay=0.0)\n",
    "dopt.zero_grad()\n",
    "gopt = optim.Adam(gen.parameters(), lr=0.0002, weight_decay=0.0)\n",
    "gopt.zero_grad()\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "target1 = torch.Tensor([1,]*batchSize + [0,]*batchSize).to(dev).view(-1,1)\n",
    "target2 = torch.Tensor([1,]*batchSize).to(dev).view(-1,1)\n",
    "\n",
    "for epoch in range(numEpoch):\n",
    "    for batch_idx,data in enumerate(trainLoader):\n",
    "        z = 2*torch.rand(data.size()[0], zdim, device=dev)-1\n",
    "        xhat = gen(z)\n",
    "\n",
    "        if batch_idx==0 and epoch==0:\n",
    "            plt.imshow(data[0,0,:,:].detach().cpu().numpy())\n",
    "            plt.savefig(\"goal.pdf\")\n",
    "            #plt.show()\n",
    "\n",
    "        if batch_idx==0 and epoch%50==0:\n",
    "            tmpimg = xhat[0:64,:,:,:].detach().cpu()\n",
    "            save_image(tmpimg, \"test_{0}.png\".format(epoch), nrow=8, normalize=True)\n",
    "            #plt.imshow(tmpimg[0,0,:,:].cpu().numpy())\n",
    "            #plt.savefig(\"test_{0}.pdf\".format(epoch))\n",
    "            #plt.ion()\n",
    "            #plt.show()\n",
    "            #plt.pause(0.001)\n",
    "\n",
    "        dopt.zero_grad()\n",
    "        for k in range(discIter):\n",
    "            logit = disc(torch.cat((data,xhat.detach()),0))\n",
    "            ##############################\n",
    "            ## implement the discriminator loss (-logD trick)\n",
    "            ##############################\n",
    "            loss = -torch.log(criterion(logit, target1))\n",
    "            print(\"E: %d; B: %d; DLoss: %f\" % (epoch,batch_idx,loss.item()))\n",
    "            loss.backward()\n",
    "            dopt.step()\n",
    "            dopt.zero_grad()\n",
    "\n",
    "        gopt.zero_grad()\n",
    "        for k in range(genIter):\n",
    "            xhat = gen(z)\n",
    "            logit = disc(xhat)\n",
    "            ##############################\n",
    "            ## implement the generator loss (-logD trick)\n",
    "            ##############################\n",
    "            loss = -torch.log(criterion(logit, target2))\n",
    "            loss.backward()\n",
    "            print(\"E: %d; B: %d; GLoss: %f\" % (epoch,batch_idx,loss.item()))\n",
    "            gopt.step()\n",
    "            gopt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
