{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.615214; ||g||: 0.613350\n",
      "Loss: 0.332295; ||g||: 0.332677\n",
      "Loss: 0.238526; ||g||: 0.237049\n",
      "Loss: 0.188502; ||g||: 0.187143\n",
      "Loss: 0.156530; ||g||: 0.155550\n",
      "Loss: 0.134098; ||g||: 0.133439\n",
      "Loss: 0.117414; ||g||: 0.116985\n",
      "Loss: 0.104489; ||g||: 0.104218\n",
      "Loss: 0.094169; ||g||: 0.094005\n",
      "Loss: 0.085731; ||g||: 0.085640\n",
      "Loss: 0.078700; ||g||: 0.078658\n",
      "Loss: 0.072749; ||g||: 0.072740\n",
      "Loss: 0.067645; ||g||: 0.067658\n",
      "Loss: 0.063218; ||g||: 0.063246\n",
      "Loss: 0.059342; ||g||: 0.059380\n",
      "Loss: 0.055918; ||g||: 0.055962\n",
      "Loss: 0.052872; ||g||: 0.052920\n",
      "Loss: 0.050145; ||g||: 0.050194\n",
      "Loss: 0.047687; ||g||: 0.047738\n",
      "Loss: 0.045462; ||g||: 0.045512\n",
      "Loss: 0.043437; ||g||: 0.043487\n",
      "Loss: 0.041586; ||g||: 0.041635\n",
      "Loss: 0.039889; ||g||: 0.039936\n",
      "Loss: 0.038325; ||g||: 0.038371\n",
      "Loss: 0.036881; ||g||: 0.036925\n",
      "Loss: 0.035542; ||g||: 0.035584\n",
      "Loss: 0.034298; ||g||: 0.034339\n",
      "Loss: 0.033139; ||g||: 0.033178\n",
      "Loss: 0.032056; ||g||: 0.032094\n",
      "Loss: 0.031043; ||g||: 0.031078\n",
      "Loss: 0.030092; ||g||: 0.030126\n",
      "Loss: 0.029198; ||g||: 0.029230\n",
      "Loss: 0.028356; ||g||: 0.028387\n",
      "Loss: 0.027561; ||g||: 0.027591\n",
      "Loss: 0.026810; ||g||: 0.026839\n",
      "Loss: 0.026100; ||g||: 0.026127\n",
      "Loss: 0.025426; ||g||: 0.025452\n",
      "Loss: 0.024786; ||g||: 0.024811\n",
      "Loss: 0.024178; ||g||: 0.024202\n",
      "Loss: 0.023600; ||g||: 0.023623\n",
      "Loss: 0.023048; ||g||: 0.023070\n",
      "Loss: 0.022522; ||g||: 0.022543\n",
      "Loss: 0.022020; ||g||: 0.022040\n",
      "Loss: 0.021539; ||g||: 0.021558\n",
      "Loss: 0.021079; ||g||: 0.021098\n",
      "Loss: 0.020639; ||g||: 0.020657\n",
      "Loss: 0.020217; ||g||: 0.020234\n",
      "Loss: 0.019811; ||g||: 0.019828\n",
      "Loss: 0.019422; ||g||: 0.019438\n",
      "Loss: 0.019048; ||g||: 0.019063\n",
      "Loss: 0.018688; ||g||: 0.018703\n",
      "Loss: 0.018341; ||g||: 0.018356\n",
      "Loss: 0.018008; ||g||: 0.018021\n",
      "Loss: 0.017686; ||g||: 0.017699\n",
      "Loss: 0.017375; ||g||: 0.017388\n",
      "Loss: 0.017075; ||g||: 0.017088\n",
      "Loss: 0.016786; ||g||: 0.016798\n",
      "Loss: 0.016506; ||g||: 0.016518\n",
      "Loss: 0.016236; ||g||: 0.016247\n",
      "Loss: 0.015974; ||g||: 0.015984\n",
      "Loss: 0.015720; ||g||: 0.015731\n",
      "Loss: 0.015475; ||g||: 0.015485\n",
      "Loss: 0.015237; ||g||: 0.015246\n",
      "Loss: 0.015006; ||g||: 0.015015\n",
      "Loss: 0.014782; ||g||: 0.014791\n",
      "Loss: 0.014565; ||g||: 0.014574\n",
      "Loss: 0.014354; ||g||: 0.014363\n",
      "Loss: 0.014150; ||g||: 0.014158\n",
      "Loss: 0.013950; ||g||: 0.013958\n",
      "Loss: 0.013757; ||g||: 0.013765\n",
      "Loss: 0.013569; ||g||: 0.013576\n",
      "Loss: 0.013386; ||g||: 0.013393\n",
      "Loss: 0.013208; ||g||: 0.013215\n",
      "Loss: 0.013034; ||g||: 0.013041\n",
      "Loss: 0.012865; ||g||: 0.012872\n",
      "Loss: 0.012701; ||g||: 0.012707\n",
      "Loss: 0.012540; ||g||: 0.012546\n",
      "Loss: 0.012384; ||g||: 0.012390\n",
      "Loss: 0.012231; ||g||: 0.012237\n",
      "Loss: 0.012082; ||g||: 0.012088\n",
      "Loss: 0.011937; ||g||: 0.011943\n",
      "Loss: 0.011795; ||g||: 0.011801\n",
      "Loss: 0.011657; ||g||: 0.011662\n",
      "Loss: 0.011522; ||g||: 0.011527\n",
      "Loss: 0.011389; ||g||: 0.011395\n",
      "Loss: 0.011260; ||g||: 0.011265\n",
      "Loss: 0.011134; ||g||: 0.011139\n",
      "Loss: 0.011011; ||g||: 0.011016\n",
      "Loss: 0.010890; ||g||: 0.010895\n",
      "Loss: 0.010772; ||g||: 0.010777\n",
      "Loss: 0.010657; ||g||: 0.010661\n",
      "Loss: 0.010543; ||g||: 0.010548\n",
      "Loss: 0.010433; ||g||: 0.010437\n",
      "Loss: 0.010324; ||g||: 0.010329\n",
      "Loss: 0.010218; ||g||: 0.010222\n",
      "Loss: 0.010114; ||g||: 0.010118\n",
      "Loss: 0.010013; ||g||: 0.010016\n",
      "Loss: 0.009913; ||g||: 0.009916\n",
      "Loss: 0.009815; ||g||: 0.009818\n",
      "Loss: 0.009719; ||g||: 0.009722\n",
      "tensor([[4.2385],\n",
      "        [0.0408]])\n"
     ]
    }
   ],
   "source": [
    "# %load A2_LogisticRegression.py\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(1)\n",
    "X = torch.Tensor([[-1, 1, 2],[1, 1, 1]])\n",
    "y = torch.Tensor([-1, 1, 1])\n",
    "w = torch.Tensor([[0.1],[0.1]])\n",
    "alpha = 1\n",
    "for iter in range(100):\n",
    "    tmp = torch.exp(torch.matmul(torch.transpose(w,0,1),X)*(-y))\n",
    "    ##############################\n",
    "    ## Use tmp to compute f and g. Instead of summing we average the result, i.e.,\n",
    "    ## complete only inside torch.mean(...) and don't remove this function\n",
    "    ## Dimensions: f (scalar); g (2)\n",
    "    ##############################\n",
    "    f = torch.mean(torch.log(1+tmp))\n",
    "    g = torch.mean((-y)*tmp/(1+tmp) * X, 1)\n",
    "    print(\"Loss: %f; ||g||: %f\" % (f, torch.norm(g)))\n",
    "    g = g.view(-1,1)\n",
    "    w = w - alpha*g\n",
    "\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.615214; ||g||: 0.613350\n",
      "Loss: 0.332295; ||g||: 0.332677\n",
      "Loss: 0.238526; ||g||: 0.237049\n",
      "Loss: 0.188502; ||g||: 0.187143\n",
      "Loss: 0.156530; ||g||: 0.155550\n",
      "Loss: 0.134098; ||g||: 0.133439\n",
      "Loss: 0.117414; ||g||: 0.116985\n",
      "Loss: 0.104489; ||g||: 0.104218\n",
      "Loss: 0.094169; ||g||: 0.094005\n",
      "Loss: 0.085731; ||g||: 0.085640\n",
      "Loss: 0.078700; ||g||: 0.078658\n",
      "Loss: 0.072749; ||g||: 0.072740\n",
      "Loss: 0.067645; ||g||: 0.067658\n",
      "Loss: 0.063218; ||g||: 0.063246\n",
      "Loss: 0.059342; ||g||: 0.059380\n",
      "Loss: 0.055918; ||g||: 0.055962\n",
      "Loss: 0.052872; ||g||: 0.052920\n",
      "Loss: 0.050145; ||g||: 0.050194\n",
      "Loss: 0.047687; ||g||: 0.047738\n",
      "Loss: 0.045462; ||g||: 0.045512\n",
      "Loss: 0.043437; ||g||: 0.043487\n",
      "Loss: 0.041586; ||g||: 0.041635\n",
      "Loss: 0.039889; ||g||: 0.039936\n",
      "Loss: 0.038325; ||g||: 0.038371\n",
      "Loss: 0.036881; ||g||: 0.036925\n",
      "Loss: 0.035542; ||g||: 0.035584\n",
      "Loss: 0.034298; ||g||: 0.034339\n",
      "Loss: 0.033139; ||g||: 0.033178\n",
      "Loss: 0.032056; ||g||: 0.032094\n",
      "Loss: 0.031043; ||g||: 0.031078\n",
      "Loss: 0.030092; ||g||: 0.030126\n",
      "Loss: 0.029198; ||g||: 0.029230\n",
      "Loss: 0.028356; ||g||: 0.028387\n",
      "Loss: 0.027561; ||g||: 0.027591\n",
      "Loss: 0.026810; ||g||: 0.026839\n",
      "Loss: 0.026100; ||g||: 0.026127\n",
      "Loss: 0.025426; ||g||: 0.025452\n",
      "Loss: 0.024786; ||g||: 0.024811\n",
      "Loss: 0.024178; ||g||: 0.024202\n",
      "Loss: 0.023600; ||g||: 0.023623\n",
      "Loss: 0.023048; ||g||: 0.023070\n",
      "Loss: 0.022522; ||g||: 0.022543\n",
      "Loss: 0.022020; ||g||: 0.022040\n",
      "Loss: 0.021539; ||g||: 0.021558\n",
      "Loss: 0.021079; ||g||: 0.021098\n",
      "Loss: 0.020639; ||g||: 0.020657\n",
      "Loss: 0.020217; ||g||: 0.020234\n",
      "Loss: 0.019811; ||g||: 0.019828\n",
      "Loss: 0.019422; ||g||: 0.019438\n",
      "Loss: 0.019048; ||g||: 0.019063\n",
      "Loss: 0.018688; ||g||: 0.018703\n",
      "Loss: 0.018341; ||g||: 0.018356\n",
      "Loss: 0.018008; ||g||: 0.018021\n",
      "Loss: 0.017686; ||g||: 0.017699\n",
      "Loss: 0.017375; ||g||: 0.017388\n",
      "Loss: 0.017075; ||g||: 0.017088\n",
      "Loss: 0.016786; ||g||: 0.016798\n",
      "Loss: 0.016506; ||g||: 0.016518\n",
      "Loss: 0.016236; ||g||: 0.016247\n",
      "Loss: 0.015974; ||g||: 0.015984\n",
      "Loss: 0.015720; ||g||: 0.015731\n",
      "Loss: 0.015475; ||g||: 0.015485\n",
      "Loss: 0.015237; ||g||: 0.015246\n",
      "Loss: 0.015006; ||g||: 0.015015\n",
      "Loss: 0.014782; ||g||: 0.014791\n",
      "Loss: 0.014565; ||g||: 0.014574\n",
      "Loss: 0.014354; ||g||: 0.014363\n",
      "Loss: 0.014150; ||g||: 0.014158\n",
      "Loss: 0.013950; ||g||: 0.013958\n",
      "Loss: 0.013757; ||g||: 0.013765\n",
      "Loss: 0.013569; ||g||: 0.013576\n",
      "Loss: 0.013386; ||g||: 0.013393\n",
      "Loss: 0.013208; ||g||: 0.013215\n",
      "Loss: 0.013034; ||g||: 0.013041\n",
      "Loss: 0.012865; ||g||: 0.012872\n",
      "Loss: 0.012701; ||g||: 0.012707\n",
      "Loss: 0.012540; ||g||: 0.012546\n",
      "Loss: 0.012384; ||g||: 0.012390\n",
      "Loss: 0.012231; ||g||: 0.012237\n",
      "Loss: 0.012082; ||g||: 0.012088\n",
      "Loss: 0.011937; ||g||: 0.011943\n",
      "Loss: 0.011795; ||g||: 0.011801\n",
      "Loss: 0.011657; ||g||: 0.011662\n",
      "Loss: 0.011522; ||g||: 0.011527\n",
      "Loss: 0.011389; ||g||: 0.011395\n",
      "Loss: 0.011260; ||g||: 0.011265\n",
      "Loss: 0.011134; ||g||: 0.011139\n",
      "Loss: 0.011011; ||g||: 0.011016\n",
      "Loss: 0.010890; ||g||: 0.010895\n",
      "Loss: 0.010772; ||g||: 0.010777\n",
      "Loss: 0.010657; ||g||: 0.010661\n",
      "Loss: 0.010543; ||g||: 0.010548\n",
      "Loss: 0.010433; ||g||: 0.010437\n",
      "Loss: 0.010324; ||g||: 0.010329\n",
      "Loss: 0.010218; ||g||: 0.010222\n",
      "Loss: 0.010114; ||g||: 0.010118\n",
      "Loss: 0.010013; ||g||: 0.010016\n",
      "Loss: 0.009913; ||g||: 0.009916\n",
      "Loss: 0.009815; ||g||: 0.009818\n",
      "Loss: 0.009719; ||g||: 0.009722\n",
      "tensor([[4.2385],\n",
      "        [0.0408]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# %load A2_LogisticRegression2.py\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "X = torch.Tensor([[-1, 1, 2],[1, 1, 1]])\n",
    "y = torch.Tensor([-1, 1, 1])\n",
    "w = torch.Tensor([[0.1],[0.1]])\n",
    "w.requires_grad = True\n",
    "alpha = 1\n",
    "\n",
    "optimizer = optim.SGD([w], lr=alpha)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "for iter in range(100):\n",
    "    \n",
    "    \n",
    "    \n",
    "    tmp = torch.exp(torch.matmul(torch.transpose(w,0,1),X)*(-y))\n",
    "\n",
    "    ##############################\n",
    "    ## loss is the same as f in A2_LogisticRegression.py\n",
    "    ## Dimensions: loss (scalar)\n",
    "    ##############################\n",
    "    loss = torch.mean(torch.log(1+tmp))\n",
    "\n",
    "    loss.backward()\n",
    "    print(\"Loss: %f; ||g||: %f\" % (loss, torch.norm(w.grad)))\n",
    "\n",
    "    ##############################\n",
    "    ## Use two functions within the optimizer instance to perform the update step\n",
    "    ##############################\n",
    "    \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShallowNet(\n",
      "  (fc1): Linear(in_features=2, out_features=1, bias=False)\n",
      ")\n",
      "tensor([0.0000, 0.2000, 0.3000], grad_fn=<SqueezeBackward0>)\n",
      "Loss: 0.615214; ||g||: 0.613350\n",
      "Loss: 0.332295; ||g||: 0.332677\n",
      "Loss: 0.238526; ||g||: 0.237049\n",
      "Loss: 0.188502; ||g||: 0.187143\n",
      "Loss: 0.156530; ||g||: 0.155550\n",
      "Loss: 0.134098; ||g||: 0.133439\n",
      "Loss: 0.117414; ||g||: 0.116985\n",
      "Loss: 0.104489; ||g||: 0.104218\n",
      "Loss: 0.094169; ||g||: 0.094005\n",
      "Loss: 0.085731; ||g||: 0.085640\n",
      "Loss: 0.078700; ||g||: 0.078658\n",
      "Loss: 0.072749; ||g||: 0.072740\n",
      "Loss: 0.067645; ||g||: 0.067658\n",
      "Loss: 0.063218; ||g||: 0.063246\n",
      "Loss: 0.059342; ||g||: 0.059380\n",
      "Loss: 0.055918; ||g||: 0.055962\n",
      "Loss: 0.052872; ||g||: 0.052920\n",
      "Loss: 0.050144; ||g||: 0.050194\n",
      "Loss: 0.047687; ||g||: 0.047738\n",
      "Loss: 0.045462; ||g||: 0.045512\n",
      "Loss: 0.043437; ||g||: 0.043487\n",
      "Loss: 0.041586; ||g||: 0.041635\n",
      "Loss: 0.039889; ||g||: 0.039936\n",
      "Loss: 0.038325; ||g||: 0.038371\n",
      "Loss: 0.036881; ||g||: 0.036925\n",
      "Loss: 0.035542; ||g||: 0.035584\n",
      "Loss: 0.034298; ||g||: 0.034339\n",
      "Loss: 0.033139; ||g||: 0.033178\n",
      "Loss: 0.032056; ||g||: 0.032094\n",
      "Loss: 0.031043; ||g||: 0.031078\n",
      "Loss: 0.030092; ||g||: 0.030126\n",
      "Loss: 0.029198; ||g||: 0.029230\n",
      "Loss: 0.028356; ||g||: 0.028387\n",
      "Loss: 0.027561; ||g||: 0.027591\n",
      "Loss: 0.026810; ||g||: 0.026839\n",
      "Loss: 0.026100; ||g||: 0.026127\n",
      "Loss: 0.025426; ||g||: 0.025452\n",
      "Loss: 0.024786; ||g||: 0.024811\n",
      "Loss: 0.024178; ||g||: 0.024202\n",
      "Loss: 0.023600; ||g||: 0.023622\n",
      "Loss: 0.023048; ||g||: 0.023070\n",
      "Loss: 0.022522; ||g||: 0.022543\n",
      "Loss: 0.022020; ||g||: 0.022040\n",
      "Loss: 0.021539; ||g||: 0.021558\n",
      "Loss: 0.021079; ||g||: 0.021098\n",
      "Loss: 0.020639; ||g||: 0.020657\n",
      "Loss: 0.020217; ||g||: 0.020234\n",
      "Loss: 0.019811; ||g||: 0.019828\n",
      "Loss: 0.019422; ||g||: 0.019438\n",
      "Loss: 0.019048; ||g||: 0.019063\n",
      "Loss: 0.018688; ||g||: 0.018703\n",
      "Loss: 0.018341; ||g||: 0.018356\n",
      "Loss: 0.018008; ||g||: 0.018021\n",
      "Loss: 0.017686; ||g||: 0.017699\n",
      "Loss: 0.017375; ||g||: 0.017388\n",
      "Loss: 0.017075; ||g||: 0.017088\n",
      "Loss: 0.016786; ||g||: 0.016798\n",
      "Loss: 0.016506; ||g||: 0.016518\n",
      "Loss: 0.016236; ||g||: 0.016247\n",
      "Loss: 0.015974; ||g||: 0.015984\n",
      "Loss: 0.015720; ||g||: 0.015731\n",
      "Loss: 0.015475; ||g||: 0.015485\n",
      "Loss: 0.015237; ||g||: 0.015246\n",
      "Loss: 0.015006; ||g||: 0.015015\n",
      "Loss: 0.014782; ||g||: 0.014791\n",
      "Loss: 0.014565; ||g||: 0.014574\n",
      "Loss: 0.014354; ||g||: 0.014363\n",
      "Loss: 0.014150; ||g||: 0.014158\n",
      "Loss: 0.013950; ||g||: 0.013958\n",
      "Loss: 0.013757; ||g||: 0.013765\n",
      "Loss: 0.013569; ||g||: 0.013576\n",
      "Loss: 0.013386; ||g||: 0.013393\n",
      "Loss: 0.013208; ||g||: 0.013215\n",
      "Loss: 0.013034; ||g||: 0.013041\n",
      "Loss: 0.012865; ||g||: 0.012872\n",
      "Loss: 0.012701; ||g||: 0.012707\n",
      "Loss: 0.012540; ||g||: 0.012546\n",
      "Loss: 0.012384; ||g||: 0.012390\n",
      "Loss: 0.012231; ||g||: 0.012237\n",
      "Loss: 0.012082; ||g||: 0.012088\n",
      "Loss: 0.011937; ||g||: 0.011943\n",
      "Loss: 0.011795; ||g||: 0.011801\n",
      "Loss: 0.011657; ||g||: 0.011662\n",
      "Loss: 0.011522; ||g||: 0.011527\n",
      "Loss: 0.011389; ||g||: 0.011395\n",
      "Loss: 0.011260; ||g||: 0.011265\n",
      "Loss: 0.011134; ||g||: 0.011139\n",
      "Loss: 0.011011; ||g||: 0.011015\n",
      "Loss: 0.010890; ||g||: 0.010895\n",
      "Loss: 0.010772; ||g||: 0.010777\n",
      "Loss: 0.010657; ||g||: 0.010661\n",
      "Loss: 0.010543; ||g||: 0.010548\n",
      "Loss: 0.010433; ||g||: 0.010437\n",
      "Loss: 0.010324; ||g||: 0.010329\n",
      "Loss: 0.010218; ||g||: 0.010222\n",
      "Loss: 0.010114; ||g||: 0.010118\n",
      "Loss: 0.010013; ||g||: 0.010016\n",
      "Loss: 0.009913; ||g||: 0.009916\n",
      "Loss: 0.009815; ||g||: 0.009818\n",
      "Loss: 0.009719; ||g||: 0.009722\n",
      "Parameter containing:\n",
      "tensor([[4.2385, 0.0408]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# %load A2_LogisticRegression3.py\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(1)\n",
    "X = torch.Tensor([[-1, 1, 2],[1, 1, 1]])\n",
    "##############################\n",
    "## modify the dataset so that it can be used here and is equivalent to the one \n",
    "## used in A2_LogisticRegression2.py and A2_LogisticRegression.py\n",
    "## Dimensions: y (3)\n",
    "##############################\n",
    "y = torch.Tensor([0, 1, 1])\n",
    "\n",
    "alpha = 1\n",
    "\n",
    "class ShallowNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ShallowNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(2,1, bias=False)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.fc1(X)\n",
    "\n",
    "net = ShallowNet()\n",
    "print(net)\n",
    "\n",
    "net.fc1.weight.data = torch.Tensor([[0.1, 0.1]])\n",
    "\n",
    "print(net(torch.transpose(X,0,1)).squeeze())\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=alpha)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "for iter in range(100):\n",
    "    netOutput = net(torch.transpose(X,0,1)).squeeze()\n",
    "\n",
    "    ##############################\n",
    "    ## provide the arguments for the criterion function\n",
    "    ##############################\n",
    "    loss = criterion(netOutput, y)\n",
    "    \n",
    "    loss.backward()\n",
    "    gn = 0\n",
    "    for f in net.parameters():\n",
    "        gn = gn + torch.norm(f.grad)\n",
    "    print(\"Loss: %f; ||g||: %f\" % (loss, gn))\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "for f in net.parameters():\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
